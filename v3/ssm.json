{
  "schemaVersion": "2.2",
  "description": "Run BigQuery â†’ S3 export in 12 parallel chunks by MONTH_ID with periodic progress logs",
  "parameters": {
    "GcpServiceAccountJson": {
      "type": "SecureString",
      "description": "GCP service account JSON (full JSON string). Store as SecureString Param and pass here."
    },
    "QueryTemplate": {
      "type": "String",
      "description": "SQL with {month_id} placeholder, e.g. SELECT * FROM `p.ds.t` WHERE MONTH_ID = {month_id}"
    },
    "BqLocation": {
      "type": "String",
      "default": "",
      "description": "Optional BigQuery job location (e.g. EU, US)"
    },
    "S3Bucket": {
      "type": "String",
      "description": "Destination S3 bucket"
    },
    "S3PrefixBase": {
      "type": "String",
      "description": "Prefix base, e.g. DATA_SOURCE_2025_10_08 (script appends _part_01.csv ... _part_12.csv)"
    },
    "MaxWorkers": {
      "type": "String",
      "default": "12",
      "description": "Parallel workers"
    },
    "Retries": {
      "type": "String",
      "default": "3"
    },
    "RetryBackoffSeconds": {
      "type": "String",
      "default": "20"
    }
  },
  "mainSteps": [
    {
      "name": "WriteFilesInstallDepsRun",
      "action": "aws:runShellScript",
      "inputs": {
        "runCommand": [
          "set -euo pipefail",
          "sudo mkdir -p /opt",
          "echo \"Writing GCP SA JSON to /opt/gcp.json\"",
          "cat > /opt/gcp.json <<'JSON_END'",
          "{{ GcpServiceAccountJson }}",
          "JSON_END",
          "chmod 600 /opt/gcp.json",
          "",
          "echo \"Writing BigQuery exporter to /opt/bq_to_s3.py\"",
          "cat > /opt/bq_to_s3.py <<'PY_END'",
          "#!/usr/bin/env python3",
          "import argparse,csv,io,os,sys,time,threading",
          "from datetime import datetime, timezone",
          "from concurrent.futures import ThreadPoolExecutor, as_completed",
          "import boto3",
          "from google.cloud import bigquery",
          "from google.oauth2 import service_account",
          "parser=argparse.ArgumentParser();",
          "parser.add_argument('--gcp-sa-json-path',required=True);",
          "parser.add_argument('--query-template',required=True);",
          "parser.add_argument('--bq-location',default=None);",
          "parser.add_argument('--s3-bucket',required=True);",
          "parser.add_argument('--s3-prefix',required=True);",
          "parser.add_argument('--max-workers',type=int,default=12);",
          "parser.add_argument('--retries',type=int,default=3);",
          "parser.add_argument('--retry-backoff-seconds',type=int,default=20);",
          "parser.add_argument('--month-column',default='MONTH_ID');",
          "parser.add_argument('--aws-region',default=os.getenv('AWS_REGION','eu-central-1'));",
          "args=parser.parse_args();",
          "session=boto3.session.Session(region_name=getattr(args,'aws-region',None) or getattr(args,'aws_region','eu-central-1'));",
          "s3=session.client('s3')",
          "credentials=service_account.Credentials.from_service_account_file(args.gcp_sa_json_path)",
          "bq_client=bigquery.Client(credentials=credentials, project=credentials.project_id, location=args.bq_location or None)",
          "TOTAL=12; done_parts=0; lock=threading.Lock(); stop=threading.Event()",
          "def pct():",
          "  with lock: return int((done_parts/TOTAL)*100)",
          "def reporter():",
          "  while not stop.is_set():",
          "    print(f\"[{datetime.now(timezone.utc).isoformat()}] progress={pct()}%\", flush=True);",
          "    stop.wait(60)",
          "import io as _io",
          "def run_month(m):",
          "  key=f\"{args.s3_prefix}_part_{m:02d}.csv\"",
          "  sql=args.query_template.format(month_id=m)",
          "  import time as _t",
          "  for attempt in range(1,args.retries+1):",
          "    try:",
          "      job=bq_client.query(sql); res=job.result(page_size=50000); schema=[f.name for f in res.schema]",
          "      buf=_io.StringIO(); import csv as _csv; w=_csv.writer(buf,lineterminator='\\n'); w.writerow(schema)",
          "      for row in res: w.writerow([row.get(c) for c in schema])",
          "      data=buf.getvalue().encode('utf-8'); s3.upload_fileobj(Fileobj=_io.BytesIO(data),Bucket=args.s3_bucket,Key=key)",
          "      print(f\"month_id={m} -> s3://{args.s3_bucket}/{key}\", flush=True)",
          "      nonlocal done_parts",
          "      with lock: done_parts+=1",
          "      return key",
          "    except Exception as e:",
          "      print(f\"ERROR month_id={m} attempt={attempt}/{args.retries}: {e}\", file=sys.stderr, flush=True)",
          "      if attempt<args.retries: _t.sleep(args.retry_backoff_seconds)",
          "      else: raise",
          "def verify(keys):",
          "  miss=[]",
          "  for k in keys:",
          "    try:",
          "      head=s3.head_object(Bucket=args.s3_bucket,Key=k)",
          "      if head.get('ContentLength',0)==0: miss.append(k)",
          "    except Exception: miss.append(k)",
          "  return miss",
          "def main():",
          "  import threading as _th",
          "  t=_th.Thread(target=reporter,daemon=True); t.start()",
          "  months=list(range(1,13)); keys=[f\"{args.s3_prefix}_part_{m:02d}.csv\" for m in months]",
          "  fails=[]",
          "  from concurrent.futures import ThreadPoolExecutor, as_completed",
          "  with ThreadPoolExecutor(max_workers=args.max_workers) as ex:",
          "    futs={ex.submit(run_month,m):m for m in months}",
          "    try:",
          "      for f in as_completed(futs):",
          "        m=futs[f]",
          "        try: f.result()",
          "        except Exception as e: fails.append((m,str(e)))",
          "    finally: stop.set(); t.join(timeout=2)",
          "  if fails:",
          "    print(f\"FAILED months: {fails}\", file=sys.stderr, flush=True); sys.exit(2)",
          "  miss=verify(keys)",
          "  if miss:",
          "    print(f\"MISSING/EMPTY files: {miss}\", file=sys.stderr, flush=True); sys.exit(3)",
          "  print(\"All 12 parts present in S3. progress=100%\", flush=True)",
          "if __name__=='__main__': main()",
          "PY_END",
          "chmod +x /opt/bq_to_s3.py",
          "",
          "echo \"Installing dependencies (google-cloud-bigquery, boto3)\"",
          "python3 -m pip install --upgrade pip >/dev/null",
          "python3 -m pip install --upgrade google-cloud-bigquery boto3 >/dev/null",
          "",
          "# Build today's date in Europe/Warsaw for the prefix if the caller passed a base without date",
          "TODAY=$(TZ=Europe/Warsaw date +%Y_%m_%d)",
          "",
          "PREFIX_BASE='{{ S3PrefixBase }}'",
          "if [[ \"$PREFIX_BASE\" != *\"_\"[0-9][0-9][0-9][0-9]_* ]]; then",
          "  PREFIX_BASE=\"${PREFIX_BASE}_${TODAY}\"",
          "fi",
          "",
          "echo \"Launching BigQuery export in parallel...\"",
          "python3 /opt/bq_to_s3.py \\",
          "  --gcp-sa-json-path /opt/gcp.json \\",
          "  --query-template \"{{ QueryTemplate }}\" \\",
          "  --bq-location \"{{ BqLocation }}\" \\",
          "  --s3-bucket \"{{ S3Bucket }}\" \\",
          "  --s3-prefix \"${PREFIX_BASE}\" \\",
          "  --max-workers {{ MaxWorkers }} \\",
          "  --retries {{ Retries }} \\",
          "  --retry-backoff-seconds {{ RetryBackoffSeconds }}"
        ]
      }
    }
  ]
}
